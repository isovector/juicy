\documentclass{article}
\usepackage{todonotes}
\usepackage{listings}
\usepackage[margin=0.5in]{geometry}


\title{CS 444 - A1}
\author{Jacob Abrahams - 20370104\\ Alexander Maguire - 20396195}

\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield,Some,None,Option},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}


\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% Default settings for code listings
\lstset{frame=tb,
  language=scala,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\begin{document}

\newcommand\type[1]{\texttt{#1}}
\newcommand\func[1]{\texttt{#1}}
\renewcommand\value[1]{\texttt{#1}}
\newcommand\source[2]{See: \texttt{src/#1.scala}::\type{#2} \\}
\newcommand\testsrc[1]{See: \texttt{test/#1.scala} \\}

\maketitle

\section{Design}

\subsection{Tokenizing}
The lexer/tokenizer we implemented uses a variant of maximal munch, trying to match each pattern of valid tokens while
consuming as much of the input as possible per token. We treat the file's contents as a stream of data, often consuming
as much as possible satisfying a given predicate. Every keyword, operator, and literal produces a corresponding token,
as do identifiers. We also produce an {\tt Invalid} token type, for when there is no valid token matching a given part
of the input, which takes an optional field denoting the exact nature of the error. For instance, when matching an
integer literal, we use the following pattern:

\begin{lstlisting}[language=Scala]
  val ipart = source.takeWhile(_.isdigit)
  if (cur.isLetter || cur == '_') {
    new Token.Invalid()
  } else if (ipart startswith "0" && ipart.length > 1) {
    new Token.Invalid(Some("Octal literals not supported"))
  } else {
    try {
      new Token.IntLiteral(ipart.toLong)
    } catch {
      case _: Throwable => new Token.Invalid(Some("Invalid integer literal "+ ipart))
    }
  }
\end{lstlisting}

\source{tokenizer/Tokenizer}{Tokenizer\#apply}

Notably, our integer literals are stored as long integers internally during the lexing step. This is due to the fact
that negative signs are parsed as their own token, so range checks can't be done until parsing. Token literals, once
produced, are assigned a {\tt SourceLocation} with their originating file, line, and column, to facilitate debugging and
helpful error message production. Tokens are then appended to a {\tt TokenStream}, which is a representation of a
tokenized file. This is similar to the {\tt CharStream} used to read in the files, but can be rewound to pre-set
bookmark locations to facilitate backtracking in the parser as needed. The tokenizer runs until the end of a file, or
the first {\tt Invalid} token produced.

\subsection{Parsing}

\subsubsection{Overview}

We decided to implement our parser with recursive-descent because we felt it would lead to a clearer, more
comprehensible code base in the final project. Additionally, we forgo building a parse tree, deciding to build the AST
directly. The parser is made up of dozens of functions, \func{parseX}, which are typed as strongly as possible,
allowing for robust testing at the unit level of every feature. In addition to the parser, we also created a set of
``parser utilities'', which perform everything that isn't directly related to building of our AST. The most important
utils which bespeckle our code are:

\begin{itemize}
    \item \func{check}: determine whether the current token is something.
    \item \func{ensure}: like \func{check}, but throws an expected error if the predicate isn't matched.
    \item \func{next}: increment the current token pointer.
    \item \func{withSource}: a macro which injects the token's \type{SourceLocation} into the generated AST node.
    \item \func{delimited}: a macro which matches one-or-more parse blocks, with a mandatory delimiter between each.
\end{itemize}
\source{parser/ParserUtils}{ParserUtils}

Additionally, many functions which should logically accept tokens will also accept a string, which internally is
tokenized and used as a token. This approach gives us type safety with our tokens, but also means we don't need to work
with them directly for many simple cases where they would provide additional readability overhead.

The following code snippit is a straight-forward example of our parser, it parses the \emph{if statement}. By greedily
matching \emph{else}s, we avoid the dangling else problem.

\begin{lstlisting}[language=Scala]
def parseIf(): IfStmnt = withSource {
  ensure("if")
  ensure("(")
  val cond = parseExpr()
  ensure(")")
  val then = parseStmnt()

  val otherwise =
    if (check("else")) {
      ensure("else")
      Some(parseStmnt())
    } else None

  new IfStmnt(cond, then, otherwise)
}
\end{lstlisting}
\source{parser/Parser}{Parser\#parseIf}



\subsubsection{Operators}

While most of the parser is straight-forward, our approach to parsing binary operators could use some explaining. We
create a list of maps from tokens to AST node constructors, ordered by ascending precedence. \func{parseExprPrec} (short
for \emph{parse expression of precedence}) will recursively call itself with higher and higher levels of precedence to
match the left-hand-side of an operator, and then matches as many of its operator tokens in the token stream, creating a
left-associative tree. This recursion automatically handles precedence for us.

There are two issues with this approach, the first is that it will always derive a left-associative tree (which is
problematic for the right-associate assignment operator), and for the ``instanceof'' operator, which does not take an
expression for a right-hand side. We handle the assignment issue by parsing it separately (since it has the lowest
precedence), but we include a hack for the ``instanceof'' case by short-circuiting the function in this case.

\source{parser/Parser}{Parser\#parseExprPrec}



\subsection{Abstract Syntax Tree}

\subsubsection{Visiting}
\label{sec:visiting}
\source{utils/Visitor}{Visitable}

Our compiler takes a relatively novel approach to visiting the AST; all syntax nodes are derived from \type{Visitable}
-- an abstract trait whose \value{children} member acts as a manifest for which nodes are visitable from this node. To
visit over the AST, \type{Visitor} is given a \textit{visit function} and \textit{folding function}. The \textit{visit
function} is given either a \value{Before(Node)} or an \value{After(Node)}, as well as all of the \value{Node}'s
ancestors in the tree to allow for context-sensitivity. The \textit{visit function} returns a generic type \type{T},
and it is the responsibility of the \textit{folding function} to transform \type{List<T>} into a single \type{T}. For
example, the \textit{visit function} might return whether or not a node is valid, and the \textit{folding function}
would return the logical AND of all the booleans. The result of this visit would then be whether or not the entire AST
is valid.

Additionally, the \value{Visitor} lifts its function arguments to provide automatic error-handling; any exceptions
deriving from \type{CompilerError} thrown in the \textit{visit function} are caught and bundled together, allowing for
natural error handling while visiting, without sacrificing encapsulation.

For a toy example, the following invocation will return a \texttt{Left(List<Error>)} if \value{ast} contains any boolean
literals, or a \value{Right(Boolean)}, whose inner value represents whether the AST contains only method definitions
(and absolutely nothing else). Such an example is obviously frivolous, but serves to illustrate the power of our
visitor.

\begin{lstlisting}[language=Scala]
ast.visit(_ && _) { (self, context) =>
  node match {
    case Before(MethodDefn(name, _, _, _, _)) =>
      println("Entering method " + name)
      true
    case After(MethodDefn(name, _, _, _, _)) =>
      println("Leaving method " + name)
      true
    case Before(BoolVal(_)) =>
      throw CompilerException("no booleans allowed")
    case _ =>
      false
  }
}
\end{lstlisting}
\source{utils/Visitor}{Visitable\#visit}



\subsection{Weeding}
\source{weeder/Weeder}{Weeder\#apply}
The weeder in our compiler uses the {\tt Visitor} trait mentioned in Section \ref{sec:visiting} to match
characteristics of nodes in the AST as necessitated by the Joos specification. This is done using pattern matching
on the case classes representing different types of tree node, applying rules to different kinds of expressions and
statements as appropriate. For instance, to verify the rule, ``A class/interface must be declared in a
.java file with the same base name as the class/interface,'' we do the following:

\begin{lstlisting}[language=Scala]
case me@ClassDefn(name, mods, extnds, impls, fields, cxrs, methods, isInterface) =>
  val basename = {
    val fname = node.originalToken.from.file.split('/').last
    if (fname endsWith ".java") fname.slice(0, fname.length - ".java".length) else ""
  }

  if (debug.checkFileName !--> (basename == name)) {
    throw new WeederError(
      s"Found class `$name` in file `$basename.java`", me)
  }
\end{lstlisting}
The Weeder maintains a list representing the current context, in essence the path from
the root file node to the current expression being parsed. This allows us to query the syntactic
location of a given {\tt Visitable}, to determine if it is contextually appropriate. For instance,
we check for explicit calls to methods in a parent class as such:

\begin{lstlisting}[language=Scala]
case me@SuperVal() =>
  if (isIn[Call](call =>
      context.contains(call.method) || me == call.method))
    throw new WeederError(
      s"Can't explicitly call methods on super()", me)
\end{lstlisting}

Every step in the weeding process either returns {\tt true}, indicating success,
or throws a {\tt WeederError}, which is caught in the enclosing {\tt Visitor}. If any
errors have been caught, they will be grouped together and re-thrown in a single
{\tt CompilerError}, which is passed to the caller. This allows the weeder to express
as many semantic errors as possible to the user, to improve fixing whatever is invalid
with the code. All errors retain the {\tt SourceLocation} of the most relevant token of the
node in the AST, to help the user find the exact location of the error.



\section{Challenges}
We faced several difficulties in building scanner and parser, most notably:
\begin{itemize}
    \item We made the decision early on in design to not build a concrete parse tree in any step of the process, jumping immediately from a token stream to an abstract syntax tree. This choice had its pros and cons, the latter including having stranger corner cases where our results did not match expectations.
    \item Partially due to the aforementioned lack of parse tree, there were a few language features we lacked in early tests, such as constructors.
    \item Due to our tokenizer immediately taking the negative sign as its own token, we could not initially do correct range checks on integer literals, as $-2147483648$ would have failed the scanner, when it is a perfectly valid integer. We fixed this by storing integers in longs initially, and doing the range check in the parser, coalescing an adjacent negative sign to the literal if present.
    \item Due to the way our weeder was structured, we could not initially detect duplicate modifiers on fields and methods (eg. \texttt{public public int x;}) This was fixed by adding logic to the parser, when building up a list of the flags applied to an identifier.
    \item Due again to the decision to solely build an AST, some structures were initially hard to parse, as they required scanning multiple tokens in a row to determine what the actual type of statement should be. An example of this would be distinguishing a variable declaration from a field assignment. We fixed this by adding checkpointing and rewinding to the token stream, so we could try to parse a series of tokens as one statement type, and, should it fail, try another type of statement, and so forth as necessary.
    \item Originally (and still, to some extent), our \type{Visitor} was implemented as a giant fold statement, returning a single boolean based on the output of the child nodes. This made error handling difficult/impossible, as there was no way to determine what caused the visitor to fail. Our initial solution to this problem was to have the Visitor instead return an \type{Either} monad containing either a boolean, or a list of strings representing the errors that accumulated. We realized this was not as clear as preferred, however, and changed our solution to just accumulate all the thrown exceptions and rethrow them all at the same time.
    \item Our solution to binary operator precedence worked well, provided the operator was left-associative and took two arbitrary expressions as operands. This didn't hold true in two cases: the assignment operator (which is right-associative), and the \func{instanceof} operator (which required the second operator to be a class name). We handled assignment well as a special case (since it has lowest priority anyway), but had to hack-in proper support for instanceof (since it's at neither end of the precedence spectrum).
\end{itemize}




\section{Testing}

We've heavily depended on a strong testing infrastructure during the development of our code; at time of writing we have
a little over 70 unit tests. Because Scala has strong support for domain specific languages, scalatest is absolutely
lovely to use. Furthermore, the testing infrastructure is a first-class citizen, and so some of our later-stage tests
are capable of succinctly running A/B tests on code -- ensuring that text substitution changes the results from pass to
failure.

Our tests have been written from the bottom-up, marking very small units to be tested. Our philosophy has been
test-driven development, and on quite a few occasions this policy has prevented us from creating regression bugs.

An example of one of our parser tests looks like this:

\begin{lstlisting}[language=Scala]
"Parser" should "right-associate assignments" in {
  mkParser("a = b = 5").parseExpr() should be ===
    Assignment(
      Id("a"),
      Assignment(Id("b"), IntVal(5)))
}
\end{lstlisting}

\testsrc{*}

\end{document}

