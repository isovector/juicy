\documentclass{article}
\usepackage{todonotes}
\usepackage{listings}

\title{CS 444 - A1}
\author{Jacob Abrahams - 20370104\\ Alexander Maguire - 20396195}

\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield,Some,None,Option},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}


\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% Default settings for code listings
\lstset{frame=tb,
  language=scala,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\begin{document}

\newcommand\type[1]{\texttt{#1}}
\newcommand\func[1]{\texttt{#1}}
\renewcommand\value[1]{\texttt{#1}}
\newcommand\source[2]{See: \texttt{#1.scala}::\type{#2} \\}

\maketitle

\section{Design}


\subsection{Overview}


\subsection{Tokenizing}
\source{tokenizer/Tokenizer}{Tokenizer\#apply}
The lexer/tokenizer we implemented uses a variant of maximal munch, trying to match each pattern of valid tokens while consuming as much of the input as possible per token. We treat the file's contents as a stream of data, often consuming as much as possible satisfying a given predicate. Every keyword, operator, and literal produces a corresponding token, as do identifiers. We also produce an {\tt Invalid} token type, for when there is no valid token matching a given part of the input, which takes an optional field denoting the exact nature of the error. For instance, when matching an integer literal, we use the following pattern:
\begin{lstlisting}[language=Scala]
  val ipart = source.takeWhile(_.isdigit)
  if (cur.isLetter || cur == '_') {
    new Token.Invalid()
  } else if (ipart startswith "0" && ipart.length > 1) {
    new Token.Invalid(Some("Octal literals not supported"))
  } else {
    try {
      new Token.IntLiteral(ipart.toLong)
    } catch {
      case _: Throwable => new Token.Invalid(Some("Invalid integer literal "+ ipart))
    }
  }
\end{lstlisting}

Notably, our integer literals are stored as long integers internally during the lexing step. This is due to the fact that negative signs are parsed as their own token, so range checks can't be done until parsing. Token literals, once produced, are assigned a {\tt SourceLocation} with their originating file, line, and column, to facilitate debugging and helpful error message production. Tokens are then appended to a {\tt TokenStream}, which is a representation of a tokenized file. This is similar to the {\tt CharStream} used to read in the files, but can be rewound to pre-set bookmark locations to facilitate backtracking in the parser as needed. The tokenizer runs until the end of a file, or the first {\tt Invalid} token produced.

\subsection{Parsing}
We decided to implement our parser with recursive-descent because we felt it would lead to a clearer, more
comprehensible code base in the final project. Additionally, we forgo building a parse tree, deciding to build the AST
directly. The parser is made up of dozens of functions, \func{parseX}, which are typed as strongly as possible,
allowing for robust testing at the unit level of every feature. In addition to the parser, we also created a set of
``parser utilities'', which perform everything that isn't directly related to building of our AST. The most important
utils which bespeckle our code are:

\begin{itemize}
    \item \func{check}: determine whether the current token is something.
    \item \func{ensure}: like \func{check}, but throws an expected error if the predicate isn't matched.
    \item \func{next}: increment the current token pointer.
    \item \func{withSource}: a macro which injects the token's \type{SourceLocation} into the generated AST node.
    \item \func{delimited}: a macro which matches one-or-more parse blocks, with a mandatory delimiter between each.
\end{itemize}
\source{parser/ParserUtils}{ParserUtils}

Additionally, many functions which should logically accept tokens will also accept a string, which internally is
tokenized and used as a token. This approach gives us type safety with our tokens, but also means we don't need to work
with them directly for many simple cases where they would provide additional readability overhead.

The following code snippit is a straight-forward example of our parser, it parses the \emph{if statement}. By greedily
matching \emph{else}s, we avoid the dangling else problem.

\begin{lstlisting}[language=Scala]
def parseIf(): IfStmnt = withSource {
  ensure("if")
  ensure("(")
  val cond = parseExpr()
  ensure(")")
  val then = parseStmnt()

  val otherwise =
    if (check("else")) {
      ensure("else")
      Some(parseStmnt())
    } else None

  new IfStmnt(cond, then, otherwise)
}
\end{lstlisting}
\source{parser/Parser}{Parser\#parseIf}



\subsection{Abstract Syntax Tree}

\subsubsection{Overview}

\subsubsection{Visiting}
\source{utils/Visitor}{Visitable}

Our compiler takes a relatively novel approach to visiting the AST; all syntax nodes are derived from \type{Visitable}
-- an abstract trait whose \value{children} member acts as a manifest for which nodes are visitable from this node. To
visit over the AST, \type{Visitor} is given a \textit{visit function} and \textit{folding function}. The \textit{visit
function} is given either a \value{Before(Node)} or an \value{After(Node)}, as well as all of the \value{Node}'s
ancestors in the tree to allow for context-sensitivity. The \textit{visit function} returns a generic type \type{T},
and it is the responsibility of the \textit{folding function} to transform \type{List<T>} into a single \type{T}. For
example, the \textit{visit function} might return whether or not a node is valid, and the \textit{folding function}
would return the logical AND of all the booleans. The result of this visit would then be whether or not the entire AST
is valid.

Additionally, the \value{Visitor} lifts its function arguments to provide automatic error-handling; any exceptions
deriving from \type{CompilerError} thrown in the \textit{visit function} are caught and bundled together, allowing for
natural error handling while visiting, without sacrificing encapsulation.


\begin{lstlisting}[language=Scala]
ast.visit(_ && _) { (self, context) =>
  node match {
    case Before(MethodDefn(name, _, _, _, _)) =>
      println("Entering method " + name)
      true

    case After(MethodDefn(name, _, _, _, _)) =>
      println("Leaving method " + name)
      true

    case _ => true
  }
}


\end{lstlisting}




\subsubsection{Operators}
\source{ast/AST}{BinaryOperator}

\subsubsection{Literals}
\source{ast/AST}{\todo{}}






\subsection{Weeding}
\source{weeder/Weeder}{Weeder\#apply}




\section{Challenges}

\begin{itemize}
    \item no grammar
    \item forgot to parse constructors
    \item negative ints
    \item duplicate modifiers
    \item rewinding to parse
    \item visitor would reduce to a single value -- hard to return errors
    \item automatic binary expression precedence; made instanceof hacky
\end{itemize}




\section{Testing}

\begin{itemize}
    \item scala test. lots of unit tests. tons! like 70
    \item most of them are built from the bottom up, so we can just test individual expressions, for example
    \item scala lets us write high-level testing infrastructure, like AB testing with keywords for the weeder
\end{itemize}

\end{document}

