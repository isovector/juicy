\documentclass{article}
\usepackage{todonotes}
\usepackage{listings}
\usepackage[margin=0.5in]{geometry}

\title{CS 444 - A4}
\author{Jacob Abrahams - 20370104\\ Alexander Maguire - 20396195}

\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield,Some,None,Option,Before,After,visit,map,foreach,flatMap},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@}, sensitive=true, morecomment=[l]{//}, morecomment=[n]{/*}{*/}, morestring=[b]",
  morestring=[b]', morestring=[b]""" }

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% Default settings for code listings
\lstset{frame=tb, language=scala, aboveskip=3mm, belowskip=3mm, showstringspaces=false, columns=flexible,
  basicstyle={\small\ttfamily}, numbers=none, numberstyle=\tiny\color{gray}, keywordstyle=\color{blue},
  commentstyle=\color{dkgreen}, stringstyle=\color{mauve}, frame=single, breaklines=true, breakatwhitespace=true
  tabsize=3 }

\begin{document}

\newcommand\type[1]{\texttt{#1}}
\newcommand\func[1]{\texttt{#1}}
\newcommand\code[1]{\texttt{#1}}
\renewcommand\value[1]{\texttt{#1}}
\newcommand\source[2]{See: \texttt{src/#1.scala}::\type{#2} \\}
\newcommand\testsrc[1]{See: \texttt{test/#1.scala} \\}

\maketitle


\section{Design}

\subsection{Resolver}
\subsubsection{Building the Package Tree}
The first step of the \type{Resolver} was to iterate over each compilation unit, and build a map from fully-qualified
typenames to their corresponding definitions. Additionally, at this step we created type definitions for primitives, and
injected them into the package tree.

The package tree provides several useful functions for working with types; for example, it allows us to get any types
which might reside in a given package (\code{import pkg.*} for example):
\begin{lstlisting}[language=Scala]
def getPackage(pkg: QName): Map[QName, TypeDefn] = {
  val len = pkg.length
  tree
    .filter(_._1.take(len) == pkg)  // same prefix
    .filter(_._1.length == len + 1) // directly inside
    .filter(_._2.isDefined)         // not a package
    .map { case (path, classDef) =>
      path.drop(len) -> classDef.get
    }
}
\end{lstlisting}
\source{ast/PackageTree}{PackageTree\#getPackage}

\subsubsection{Determining Type Scopes}
For each compilation unit (internally referred to as a \type{FileNode}), we constructed a ``type scope'' -- a map from a
single identifier to any types it might resolve to -- by pulling in references from the package tree. Type scopes are
written into the AST, so later stages can take advantage of this analysis.

\subsubsection{Resolving Types}
The \type{Resolver} then performs the actual type resolution of syntactically-available types. This is implemented via a
visitor over each \type{FileNode} whose purpose is to find every \type{Typename} existing in the AST and to point it
towards its definition from the package tree. Any \type{Typename} which fails to resolve unambiguously via the type
scope results in an error.



\subsection{Knower}
\subsubsection{Inheritance Expansion}
The \type{Knower} is responsible for hierarchy checking. The most interesting part of this pass is its flattening of
class hierarchies: every class pulls all of its parent's methods into scope, respecting hiding rules. This is
implemented via a lazy recursion over a class' resolved \code{extends}.

In order to get all classes deriving from \type{java.lang.Object}, the \type{Parser} was extended with a special case to
make any class without an explicit \code{extends} clause extend from \type{java.lang.Object}.

\newpage
\begin{lstlisting}[language=Scala]
lazy val (inheritedMethods: Seq[MethodDefn], hidesMethods: Seq[MethodDefn]) = {
  val parentMethods =
    extnds
      .flatMap(
        _.resolved.get.inheritedMethods)
      .filter(!_.isCxr)
  val sigs =
    methods
      .map(_.signature)
  val (hides, keeps) =
    parentMethods
      .partition { parMeth =>
        sigs.contains(parMeth.signature)
      }

  (methods ++ keeps, hides)
}
\end{lstlisting}
\source{ast/AST}{TypeDefn}

\subsubsection{Interface Contract Verification}
Additionally, the \type{Knower} is responsible for determining whether classes satisfy their \code{implements}
contracts; similarly to the inheritance expansion stage, this is implemented by flattening the implementation hierarchy,
and ensuring every class has method signatures which agree with every interface in the hierarchy.

\source{resolver/HardlyKnower}{HardlyKnower\#apply:84}




\subsection{Scoper}
The scope uses the Visitor pattern to build up a symbol table for each lexical scope. It does so in a tree-like structure,
where lexically nested scopes are children of their enclosing scope, and the root node of every scope is the scope associated
with the enclosing class. Further, the scoper handles variable name clashes, specifically between local variables and parameters
and different fields defined in the same class. It then appends the scope as a field to the AST node in question for use in
typechecking. However, due to the structure, it builds a single symbol table for the entire scope, and thus cannot differentiate
between names already in scope and names that have not yet been declared. This necessitated a later pass to detect use-before-declaration
conflicts. Further, the scoper didn't actually detect names that didn't exist in scope when used, as it wasn't until a later rewriting
pass that we could differentiate between package names, member usages, and local variables. This limited scope of functionality did
result in an extremely clean, concise pass, as shown in the examples below:

\begin{lstlisting}[language=Scala]
node.visit { (self, context) =>
  self match {
    // ... other cases
    case Before(VarStmnt(name,_,tname,_)) =>
      if (curBlock != curClass && !curBlock.define(name, tname)) {
        // Already defined
        throw new ScopeError("Duplicate definition of variable \$name", self.from)
      }
    case Before(_: BlockStmnt) =>
      makeChildScope()
    case After(_: BlockStmnt) =>
      freeChildScope()
    case Before(_: WhileStmnt) =>
      makeChildScope()
    case After(_: WhileStmnt) =>
      freeChildScope()
  }
}
\end{lstlisting}
\source{scoper/Scoper}{Scoper\#apply}



\subsection{Disambiguator}
\subsubsection{Rewriter}
By the time we had the need to disambiguate names, we ran into an issue where our AST no longer provided semantic
information about what we were trying to accomplish. For example, static method calls would be represented in the AST
by:

\begin{lstlisting}[language=Scala]
val staticCall = parse("pkg.Type.method()")
staticCall === Call(Member(Member(Id("pkg"), Id("Type")), Id("method")))
\end{lstlisting}

Notably, this doesn't reflect that \code{pkg.Type} is not a member access, but instead refers to a single ontological
entity (the type \type{pkg.Type}). Any further passes over the AST would require re-implementing parsing this chained
member access into the intended type resolution. It was decided that instead we should rewrite the AST to elide this
issue.

Unfortunately, our generalized visitor strategy (described in \textit{A1}) turned out to have insufficient power to
implement AST rewriting. A brief review of the literature suggested a technique called \textit{Scrap Your Boilerplate}
(SYB), to implement this for arbitrary trees, however, the implementation details of this strategy were deemed to be
outside the scope of our project (plus, monads are scary).

Instead, we implemented a \func{rewrite} method on every node of the AST, who was responsibile for rewriting all of its
children, constructing a new version of itself (since our AST is mostly immutable), and then rewriting this new copy of
itself. An example of this pattern is shown here:

\begin{lstlisting}[language=Scala]
def rewrite(rule: Rewriter, context: Seq[Visitable]) = {
  val newContext = this +: context
  transfer(rule(
    IfStmnt(
      cond.rewrite(rule, newContext).asInstanceOf[Expression],
      then.rewrite(rule, newContext).asInstanceOf[BlockStmnt],
      otherwise.map(_.rewrite(rule, newContext).asInstanceOf[BlockStmnt])
    ), context))
}
\end{lstlisting}
\source{ast/AST}{IfStmnt\#rewrite}

\func{transfer} is a function which keeps metadata on the AST (for example, original source location and scope), while
\func{rule} is a partial function defined over AST nodes which returns what that node should be replaced with. To simply
visit the tree, the identity function will suffice as a \func{rule}. Unfortunately, this approach loses type-safety,
which isn't a huge issue in practice, but did make us squirm a little.


\subsubsection{Static Disambiguation}
With the \type{Rewriter} in place, it was now possible to rewrite chained member accesses representing qualified
types with static member accesses. Another pass of the AST is performed here to resolve all remaining types (those which
are not syntactically-available).

\begin{lstlisting}[language=Scala]
node.rewrite(Rewriter { (newNode: Visitable, context: Seq[Visitable]) =>
  newNode match {
    case m: Member =>
      val folded =
        m.fold {
          case id: Id if id.status != SCOPE => Some(id)
          case _ => None
        }
      if (!folded.contains(None)) {
        val rhs = folded.flatten.last
        val path = folded.flatten.dropRight(1)
        val qname = path.map(_.name)

        if (path.last.status == TYPE) {
          val classDefn = node.resolve(qname, pkgtree, m.from).get
          StaticMember(classDefn.asInstanceOf[ClassDefn], rhs)
        } else {
          disambiguate(rhs, qname)
          m
        }
      } else m
  }
}
\end{lstlisting}
\source{disambiguator/Disambiguator}{Disambiguator\#apply}



\subsection{Analysis Probe}
The \type{AnalysisProbe} is responsible for performing static analysis over the AST; it determines whether all
statements are reachable and if all code paths necessarily return values. Additionally, since it is already walking the
control flow graph, we augment its responsibilities with checking whether or not variables are in scope (since the
\type{Scoper} only determines this on a per-block level).

An excerpt of our probing function is presented here to demonstrate the approach taken to walk the control flow graph:

\begin{lstlisting}[language=Scala]
private def probe(reachable: Boolean, stmnt: Statement): Boolean = {
  if (!reachable)
    throw UnreachableError(stmnt)

  stmnt match {
    case BlockStmnt(stmnts) =>
      (true /: stmnts)(probe)
    case IfStmnt(_, then, otherwise) =>
      if (otherwise.isDefined)
        probe(then) || probe(otherwise.get)
      else {
        probe(then)
        true
      }
    case WhileStmnt(cond, body) =>
      cond match {
        case BoolVal(true) => false
        case BoolVal(false) => probe(false, body)
        case _ => true
      }
    // ... other cases
\end{lstlisting}
\source{analysis/AnalysisProbe}{AnalysisProbe\#probe}



\subsection{Checker}
The Checker took the duty of checking expression types for validity, member field usages for proper accesses, and some aspects of constant folding.
As some nodes needed to be modified to take advantage of constant expressions, this was done in a rewriter to facilitate replacing nodes as needed.

\subsubsection{Typechecking}
Typechecking was done by recursively assigning types to AST nodes, then resolving enclosing nodes based on child node types. For variables,
this was done by reading the scope struct associated for a node. For function returns, this was dones by scanning through the list of
all methods available to a particular classs, and seeing if a call with exactly matching parameter types existed. For subexpressions with
unambiguous type (eg. \texttt{instanceof}), type was assigned even if the type of the operands was invalid or unknown, to maximize the
helpfulness of generated error messages. Otherwise, if any subexpression was of invalid type, the entire expression would not propogate
a type, so as to minimize useless or repetitive errors for the same expression. For example, consider the case of array indexing:

\begin{lstlisting}[language=Scala]
case ind: Index =>
  if (ind.lhs.exprType.isEmpty || ind.rhs.exprType.isEmpty) {
    // Nothing to do
  } else if (!(numerics contains ind.rhs.exprType.get)) {
    // Indexing with non-numeric type
    errors :+= unsupported("[]", ind.from, ind.lhs.exprType.get, ind.rhs.exprType.get)
  } else {
    // Check to make sure LHS is an array
    val t = ind.lhs.exprType.flatMap(_.resolved).get
    t match {
      case arr@ArrayDefn(elem) => ind.exprType = Some(elem.makeTypename)
      case _ => errors :+= unsupported("[]", ind.from, ind.lhs.exprType.get, ind.rhs.exprType.get)
    }
  }
  ind
\end{lstlisting}

\subsection{Access Checking}
To check access, we used the following two functions based off of the Java Language Specification:
\begin{lstlisting}[language=Scala]
def hasStaticProtectedAccess(t1: TypeDefn, tref: TypeDefn, tdef: TypeDefn): Boolean = {
  return ((t1 isSubtypeOf tdef)) || (t1.pkg == tdef.pkg)
}
def hasInstanceProtectedAccess(t1: TypeDefn, tref: TypeDefn, tdef: TypeDefn): Boolean = {
  return ((tref isSubtypeOf t1) && (t1 isSubtypeOf tdef)) || (t1.pkg == tdef.pkg)
}
\end{lstlisting}
\value{t1} is the type of the accessing class (in effect, the type of \texttt{this}), \value{tref} is the compile-time
class/interface of the object whose field is being accessed, and \value{tdef} is the type in which the field or method was
actually defined. The choice of function to call was based off of whether or not the access was in a static context, which
was determined syntactically when possible (eg, qualified static members of classes, qualified instance members of variables),
and determined from the declaration itself whenever necessary (eg. unqualified uses).

\subsubsection{Constant Folding}
Constant folding was done on a per-expression basis, detecting expressions containing only literals and returning an equivalent
AST node appropriately. This was done for the \texttt{+} operation for string literals, every numerical operation, including
\texttt{+}, for numeric literals, and boolean expressions for operands of any type. Further, if \type{D} was derived from type \type{B},
the expression \texttt{d instanceof B} for an object \texttt{d} of declared type \type{D} would be replaced with a simple boolean literal
\texttt{true}, as this always has to be the case if the source has yet to emit an exception by this point in execution. Some constant folding
was left out due to the quadratic nature of the combinations of operand types, most notably concatenating string literals with \value{null}
or numeric literals, as this was unnecessary for determining reachability, and thus would only be implemented to simplify some of code
generation later on. As an example of what \emph{was} implemented, consider the following function used to fold comparisons between 
numeric literals into an equivalent boolean true or false, with \func{fold} being a lambda doing the actual equivalent comparison:

\begin{lstlisting}[language=Scala]
 def doComp(expr: BinOp, fold: (Int, Int) => Boolean, symbol: String): Expression = {
  if (expr.lhs.exprType.isEmpty || expr.rhs.exprType.isEmpty) {
    // There was already an error, don't bother trying to determine types
    expr
  } else if ((numerics contains expr.lhs.exprType.get) && (numerics contains expr.rhs.exprType.get)) {
   val newExpr = (expr.lhs, expr.rhs) match {
      case (l: CharVal, r: CharVal) => BoolVal(fold(l.value, r.value))
      case (l: CharVal, r: IntVal) => BoolVal(fold(l.value, r.value))
      case (l: IntVal, r: CharVal) => BoolVal(fold(l.value, r.value))
      case (l: IntVal, r: IntVal) => BoolVal(fold(l.value, r.value))
      case _ => expr // At least one non-literal, so don't fold
    }
    newExpr.exprType = Some(BoolTypename)
    newExpr
  } else {
    // Comparison between non-numeric types
    errors :+= unsupported(symbol, expr.from, expr.lhs.exprType.get, expr.rhs.exprType.get)
    expr
  }
}
\end{lstlisting}

\source{checker/Checker}{Checker\#apply}



\section{Challenges}
\begin{itemize}
    \item Our resolving step had to be rewritten twice; the first time we realized that we had passed all of the a2
        tests, but some of them were for the wrong reasons. Instead of trying to figure out why (which we considered to
        be a dangerous undertaking), it was decided that we rewrite the logic entirely from first principles. The second
        time was when we realized that syntactically-ambiguous types needed equivalent logic for resolution, but our
        current solution was unable of handling this. As it currently stands, much of the resolving logic is not handled
        directly by the \type{Resolver} itself, but exists in the \type{FileNode} so we have access to it in later
        stages.
    \item In addition to the above issues, we found that some of our \type{Resolver} unit tests would hang indefinitely.
        Tracking down this bug led to the realization that our types were being compared by value (which was good), but
        after being resolved could contain cyclic references (which was bad). This was ``solved'' by writing a custom
        equality method for our types, but broke our unit tests which had facilities to handle the new equality logic.
        In order to fix \textit{that}, we added a means of turning off the extra equality handling for these unit tests,
        but this solution turned out to be non-reentrant, leading to non-deterministic failures in deterministic unit
        tests. In the end, we simply turned off parallel execution of unit tests.
   \item Up until this point, primitives and arrays were handled by the syntax-tree builder in a less-than-optimal way,
         doing a check against the name for primitives and having a readable ``isArray'' field on the typename. Due to
         the nature of typechecking, we needed to handle these non-Object types in a more robust manner, and thus had to
         use a rewriter to resolve array types to a special, never-actually-generated-in-the-parser kind of class definition
         called \type{ArrayDefn}, and resolve primitive keywords to a similar internal-only \type{TypeDefn} called \type{PrimitiveDefn}
   \item The scoper was agnostic to the order of declared variables, as it only had a concept of block- and class-level
         scope due to the tree-like structure used to implement it. As such, use-before-definition errors were difficult
         to detect as-is, and some of the code had to be restructured to allow for this necessary code structure check.
   \item The checker arguably spiralled out of control, in that it tried to do too many things at once, including type-checking,
         cast validation, static and instance member resolution, protected access checking, and some of the simpler
         expression folding. It may have been wiser to split it into multiple separate rewriters, each doing one or two of
         the aforementioned operations, which would have not resulted in the monolithic 600-line pattern match that the
         type checker ended up being. Further, the multiple duties of the type checker necessitated the return of multiple
         errors at once, which the rewriter base was unable to do, unlike the visitor base. This meant that the same
         error-folding behaviour had to be folded into the function itself, leading to marginally less clean code.
    \item Null literals were difficult to handle, as they theoretically had a subtype of anything that subclassed (or could potentially
          subclass) Object, but were unrelated to primitives. This was handled by giving them their own \type{TypeDefn}
          with no methods or fields, and special-casing in the type-checker for the generated \type{NullType}, particularly for
          assignability, \func{instanceof} checks, and equality comparisons. Further, types were annotated with a \value{nullable}
          field, which was defined as \value{false} for primitive types and \value{true} for everything else.
\end{itemize}




\section{Testing}

As in our documentation for a1, we have remained heavily dependent on a strong unit-testing infrastructure. Most
compiler passes have approximately 10 unit tests written for them, with some of the trickier ones having over 40. In
addition, we have integrated marmoset's tests into our continuous integration tests, giving us an accurate notion of how
well we are doing on the assignment. As an added bonus, we also get a huge repository of tests from previous assignments
to ensure we haven't introduced any regressions as we proceed.

\testsrc{*}

\end{document}

